{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b48288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "import src.svm_utils as svm_utils\n",
    "import src.visualization_utils as viz_utils\n",
    "import src.ds_utils as ds_utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766f7508",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLUE = sns.color_palette(\"tab10\")[0]\n",
    "RED = sns.color_palette(\"tab10\")[3]\n",
    "ORANGE = sns.color_palette(\"tab10\")[1]\n",
    "BROWN = sns.color_palette(\"tab10\")[5]\n",
    "GRAY = sns.color_palette(\"tab10\")[7]\n",
    "GREEN = sns.color_palette(\"tab10\")[2]\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 12,\n",
    "          'figure.figsize': (5, 3),\n",
    "         'axes.labelsize': 14,\n",
    "         'axes.titlesize':16,\n",
    "         'xtick.labelsize':14,\n",
    "         'ytick.labelsize':14}\n",
    "pylab.rcParams.update(params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb4598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "beton_root = \"/mnt/cfs/projects/correlated_errors/betons\"\n",
    "experiment_root = \"/mnt/cfs/projects/correlated_errors/experiments/spurious_cifar100/unlabeled_1_4_new_spurious_norm\"\n",
    "\n",
    "svm_name = \"svm_spurious_unlabeled_normalized\"\n",
    "name = os.path.join(experiment_root, f\"svm_checkpoints/{svm_name}.pt\") # SVM output file\n",
    "svm_model_name = os.path.join(experiment_root, f\"svm_checkpoints/{svm_name}_model.pkl\") # SVM output file\n",
    "model_root = os.path.join(experiment_root, \"models\")\n",
    "model_ckpt = os.path.join(model_root, \"spurious_supercifar100_unlabeled/version_0/checkpoints/checkpoint_last.pt\")\n",
    "loss_upweight_root = os.path.join(experiment_root, \"loss_vec_files\")\n",
    "subset_root = os.path.join(experiment_root, \"subset_index_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d54da",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = viz_utils.SVMProcessor(name, root=beton_root, checkpoint_path=model_ckpt, get_unlabeled=True)\n",
    "classes_to_drop = torch.load(processor.metrics['args']['indices_file'])['classes_to_drop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fc9d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724c07df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55535cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.pytorch_datasets as pytorch_datasets\n",
    "ds = pytorch_datasets.SuperCIFAR100(root=\"/mnt/nfs/home/saachij/datasets/cifar100\", train=False)\n",
    "class_names = np.array(ds.classes)\n",
    "subclass_names = []\n",
    "# asterisk subclass_names\n",
    "for c, n in enumerate(ds.subclasses):\n",
    "    name = ' '.join(n.split('_'))\n",
    "    if c in classes_to_drop:\n",
    "        name += \"*\"\n",
    "    subclass_names.append(name)\n",
    "subclass_names = np.array(subclass_names)\n",
    "singular_class_names = ['aquatic mammal', 'fish', 'flower', 'food container', 'fruit or vegetable', 'household electrical device', 'household furniture', 'insect', 'large carnivore', 'large man-made outdoor thing', 'large natural outdoor scene', 'large omnivores and herbivore', 'medium-sized mammal', 'non-insect invertebrate', 'person', 'reptile', 'small mammal', 'tree', 'standard vehicle', 'specialized vehicle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd6a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cross val scores \n",
    "import torch\n",
    "import numpy as np\n",
    "def check_efficacy(split='test', is_correct=None):\n",
    "    superclass = processor.metrics[f'{split}_metrics']['classes'] # 0 if female, 1 if male\n",
    "    subclass = processor.metrics[f'{split}_metrics']['spuriouses'] #1 if blond, 2 if black hair, 0 if neither\n",
    "    if is_correct is None:\n",
    "        is_correct = processor.metrics[f'{split}_metrics']['ytrue']\n",
    "    min_classes = []\n",
    "    for c in np.unique(superclass):\n",
    "        mask = superclass == c\n",
    "        print(f\"---{c}---\")\n",
    "        class_accs = []\n",
    "        subc_list = np.unique(subclass[mask])\n",
    "        for c2 in subc_list:\n",
    "            if c2 in classes_to_drop:\n",
    "                suffix=\"*\"\n",
    "            else:\n",
    "                suffix = \"\"\n",
    "            mask2 = subclass == c2\n",
    "            acc = is_correct[mask & mask2].mean()\n",
    "            class_accs.append(acc)\n",
    "            num = len(is_correct[mask & mask2])\n",
    "            print(f\"{c2}{suffix}, {acc:0.4f}, {num}\")\n",
    "        min_classes.append(subc_list[np.argmin(class_accs)])\n",
    "    print(min_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2378fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"val\")\n",
    "test_masks = check_efficacy(\"val\")\n",
    "print(\"\\ntest\")\n",
    "test_masks = check_efficacy(\"test\")\n",
    "print(\"\\ntrain\")\n",
    "train_masks = check_efficacy(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fc175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split = 'test'\n",
    "# for f in [\n",
    "#     \"runs/spurious_soft/version_0/metrics.pt\", \n",
    "#     \"runs/spurious_overweight2/version_0/metrics.pt\",\n",
    "#     \"runs/spurious_oracle2/version_0/metrics.pt\",\n",
    "# ]:\n",
    "#     print(f)\n",
    "#     fix = torch.load(f)\n",
    "#     print(fix[split]['Accuracy'])\n",
    "#     is_correct = fix[split]['preds'] == fix[split]['classes']\n",
    "#     check_efficacy(split, is_correct.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4184f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'test'\n",
    "test_dv = processor.metrics[f'{split}_metrics']['decision_values']\n",
    "test_confs = processor.run_dict[split]['confs']\n",
    "test_superclass = processor.metrics[f'{split}_metrics']['classes'] # 0 if female, 1 if male\n",
    "test_subclass = processor.metrics[f'{split}_metrics']['spuriouses'] #1 if blond, 2 if black hair, 0 if neither\n",
    "test_problematic = np.in1d(test_subclass, classes_to_drop)\n",
    "test_pred_correct = processor.metrics[f'{split}_metrics']['ypred']\n",
    "test_correct = processor.metrics[f'{split}_metrics']['ytrue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245080a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sklearn_metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "conf_matrix = sklearn_metrics.confusion_matrix(y_true=(test_problematic == 0), y_pred=test_correct)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "sns.heatmap(conf_matrix/conf_matrix.sum(), fmt='.2%', annot=True, cmap='Blues')\n",
    "ax.yaxis.set_ticklabels(['Problematic', 'Not Problematic'])\n",
    "ax.xaxis.set_ticklabels(['Incorrect', 'Correct'])\n",
    "perc_incorr_that_is_prob = (test_problematic & (test_correct == 0)).sum()/(test_correct == 0).sum()\n",
    "perc_prob_that_is_incorr = (test_problematic & (test_correct == 0)).sum()/(test_problematic).sum()\n",
    "print(f\"percentage incorrect that is problematic {perc_incorr_that_is_prob:0.3%}\")\n",
    "print(f\"percentage problematic that is incorrect {perc_prob_that_is_incorr:0.3%}\")\n",
    "plt.show()\n",
    "\n",
    "conf_matrix = sklearn_metrics.confusion_matrix(y_true=(test_problematic == 0), y_pred=test_pred_correct)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "sns.heatmap(conf_matrix/conf_matrix.sum(), fmt='.2%', annot=True, cmap='Blues')\n",
    "ax.yaxis.set_ticklabels(['Problematic', 'Not Problematic'])\n",
    "ax.xaxis.set_ticklabels(['Predicted Incorrect', 'Predicted Correct'])\n",
    "perc_incorr_that_is_prob = (test_problematic & (test_pred_correct == 0)).sum()/(test_pred_correct == 0).sum()\n",
    "perc_prob_that_is_incorr = (test_problematic & (test_pred_correct == 0)).sum()/(test_problematic).sum()\n",
    "print(f\"percentage flagged that is problematic {perc_incorr_that_is_prob:0.3%}\")\n",
    "print(f\"percentage problematic that is flagged {perc_prob_that_is_incorr:0.3%}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ec67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as scipy_stats\n",
    "import pandas as pd\n",
    "def compute_entropy(arr):\n",
    "    _, counts = np.unique(arr, return_counts=True)\n",
    "    return scipy_stats.entropy(counts/counts.sum())\n",
    "\n",
    "df = []\n",
    "for c in range(20):\n",
    "    mask = test_superclass == c\n",
    "    N = len(test_dv[mask])\n",
    "    dv_order = np.argsort(test_dv[mask])\n",
    "    conf_order = np.argsort(test_confs[mask])\n",
    "\n",
    "    for K in range(10, N, 5):\n",
    "        df.append([\n",
    "            c,\n",
    "            K,\n",
    "            compute_entropy(test_subclass[mask][dv_order[:K]]),\n",
    "            compute_entropy(test_subclass[mask][conf_order[:K]]),\n",
    "            compute_entropy(test_subclass[mask]),\n",
    "        ])\n",
    "df = pd.DataFrame(df, columns=['class', 'Top K', 'SVM', 'Confidence', 'All'])\n",
    "df = df.melt(['class', 'Top K'], var_name='Method', value_name='Entropy')\n",
    "fig, ax =plt.subplots(1, 1, figsize=(5, 3))\n",
    "sns.lineplot(data=df, x='Top K', y='Entropy', hue='Method', ax=ax, ci=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab9e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as scipy_stats\n",
    "import pandas as pd\n",
    "def compute_fraction(arr):\n",
    "    return arr.sum()/len(arr)\n",
    "\n",
    "\n",
    "df = []\n",
    "for c in range(20):\n",
    "    mask = test_superclass == c\n",
    "    N = len(test_dv[mask])\n",
    "    dv_order = np.argsort(test_dv[mask])\n",
    "    conf_order = np.argsort(test_confs[mask])\n",
    "\n",
    "    for K in range(10, N, 5):\n",
    "        df.append([\n",
    "            c,\n",
    "            K,\n",
    "            compute_fraction(test_problematic[mask][dv_order[:K]]),\n",
    "            compute_fraction(test_problematic[mask][conf_order[:K]]),\n",
    "            compute_fraction(test_problematic[mask]),\n",
    "        ])\n",
    "df = pd.DataFrame(df, columns=['class', 'Top K Flagged', 'SVM Decision Value', 'Confidence', 'Base Population'])\n",
    "df = df.melt(['class', 'Top K Flagged'], var_name='Order', value_name='Fraction Minority Subclass')\n",
    "fig, ax =plt.subplots(1, 1, figsize=(8, 4))\n",
    "sns.lineplot(data=df, x='Top K Flagged', y='Fraction Minority Subclass', hue='Order', ax=ax, ci=None,\n",
    "             hue_order=['SVM Decision Value', 'Confidence', 'Base Population'], palette=[BLUE, RED, GRAY])\n",
    "# handles, labels = ax.get_legend_handles_labels()\n",
    "# ax.legend(handles=handles, labels=labels)\n",
    "os.makedirs(\"figures/spurious_cifar100\", exist_ok=True)\n",
    "plt.savefig(\"figures/spurious_cifar100/frac_pop.pdf\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16c1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['dv'] = test_dv\n",
    "df['confs'] = test_confs\n",
    "df['superclass'] = test_superclass\n",
    "df['superclass_name'] = class_names[test_superclass]\n",
    "df['subclass'] = test_subclass\n",
    "df['subclass_name'] = subclass_names[test_subclass]\n",
    "df['is_corrects'] = test_correct\n",
    "df = df.sort_values('subclass_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba3beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(20, 3, figsize=(15, 40))\n",
    "for c in range(20):\n",
    "    mask = df['superclass'] == c\n",
    "    sns.violinplot(data=df[mask], x='subclass_name', y='dv', ax=ax[c, 0])\n",
    "    ax[c, 0].set_xlabel(None)\n",
    "    ax[c, 0].set_ylabel(\"SVM Decision Value\")\n",
    "    sns.violinplot(data=df[mask], x='subclass_name', y='confs', ax=ax[c, 1])\n",
    "    ax[c, 1].set_xlabel(None)\n",
    "    ax[c, 1].set_ylabel(\"Confidence\")\n",
    "    sns.barplot(data=df[mask], x='subclass_name', y='is_corrects', ax=ax[c, 2])\n",
    "    ax[c, 2].set_xlabel(None)\n",
    "    ax[c, 2].set_ylabel(\"Accuracy\")\n",
    "    ax[c, 2].set_title(class_names[c])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09384de1",
   "metadata": {},
   "source": [
    " # CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f872ef5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import src.clip_utils as clip_utils\n",
    "clip_analyzer = clip_utils.ClipAnalyzer(\n",
    "    processor=processor, svm_model_name=svm_model_name, class_names=singular_class_names,\n",
    "    clip_config_name='CIFAR100', do_normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9191de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cdf(arr, K_range=None):\n",
    "    out = []\n",
    "    if K_range is None:\n",
    "        K_range = np.arange(10, len(arr), 10)\n",
    "    for K in K_range:\n",
    "        out.append(arr[:K].mean())\n",
    "    out = np.array(out)\n",
    "    return out, K_range\n",
    "\n",
    "test_class = test_superclass\n",
    "saved_caption_and_most_relevant_imgs = {}\n",
    "df_dict = {}\n",
    "for METHOD in ['CLASSIFY']:\n",
    "    all_dfs = []\n",
    "    for target_class in range(20):\n",
    "        print(processor.metrics['cv_scores'][target_class])\n",
    "\n",
    "        if METHOD == 'CLOSEST':\n",
    "            print(\"performing closest\")\n",
    "            result = clip_analyzer.perform_closest_to_top_K(target_class, 'all')\n",
    "        else:\n",
    "            print(\"performing classify captions on svm\")\n",
    "            result = clip_analyzer.get_svm_style_top_K(target_class, 'all')\n",
    "        print(\"--------\")\n",
    "\n",
    "        cdfs = {}\n",
    "        class_mask = test_class==target_class\n",
    "        masked_indices = np.arange(len(test_class))[class_mask]\n",
    "        # K_range = np.arange(10, len(masked_indices), 10)\n",
    "        K_range=np.arange(20, 200, 5)\n",
    "        for caption_index in range(2):\n",
    "            for direction in ['pos', 'neg']:\n",
    "                caption_text = result[f'{direction}_captions'][caption_index]\n",
    "                print(f\"{direction}: {caption_text}\")\n",
    "                top_caption_latent = torch.tensor(result[f'{direction}_latents'][caption_index]).cuda()\n",
    "\n",
    "                image_latents = clip_analyzer.clip_features['test'][class_mask].cuda()\n",
    "                image_angles = clip_utils.order_descriptions_angle(mean_point=top_caption_latent.unsqueeze(0), query_points=image_latents)\n",
    "                image_order = np.argsort(image_angles)[::-1]\n",
    "                saved_caption_and_most_relevant_imgs[(METHOD, target_class, caption_index, direction)] = (masked_indices[image_order], caption_text)\n",
    "                cdfs[direction], _ = get_cdf(test_correct[masked_indices[image_order]], K_range)\n",
    "                # uncomment this to display the images\n",
    "#                 processor._display_images(taken_index=masked_indices[image_order], taken_scores=image_angles[image_order],\n",
    "#                             taken_confs=image_angles[image_order], split=\"test\")\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            df['K'] = K_range\n",
    "            for d, v in cdfs.items():\n",
    "                df[d] = v\n",
    "            df = df.melt('K', var_name='Direction', value_name='Accuracy')\n",
    "            all_dfs.append(df)\n",
    "            sns.lineplot(data=df, x='K', y='Accuracy', hue='Direction')\n",
    "            plt.axhline(y=test_correct[masked_indices].mean(), xmin=0, xmax=K_range[-1], color='gray')\n",
    "            plt.show()\n",
    "    df_dict[METHOD] = all_dfs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e3867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subclass_names[classes_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(20):\n",
    "    print(\"--\")\n",
    "    print(subclass_names[classes_to_drop][c])\n",
    "    for i in range(2):\n",
    "        print(saved_caption_and_most_relevant_imgs[('CLASSIFY', c, i, 'neg')][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba4f50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display extremes for original model\n",
    "for c in range(20):\n",
    "    processor.display_extremes(c, split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b123da00",
   "metadata": {},
   "source": [
    "## Interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac9bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'\n",
    "orders = processor.orders['train']['SVM'][True] # most negative first\n",
    "dv = processor.metrics[f'{split}_metrics']['decision_values']\n",
    "superclass = processor.metrics[f'{split}_metrics']['classes']\n",
    "subclass = processor.metrics[f'{split}_metrics']['spuriouses'] #1 if blond, 2 if black hair, 0 if neither\n",
    "problematic = np.in1d(subclass, classes_to_drop)\n",
    "def visualize_loss_vec(loss_vec):\n",
    "    for c in np.unique(superclass):\n",
    "        mask = superclass == c\n",
    "        print(f\"---{c}---\")\n",
    "        for c2 in np.unique(subclass[mask]):\n",
    "            if c2 in classes_to_drop:\n",
    "                suffix=\"*\"\n",
    "            else:\n",
    "                suffix = \"\"\n",
    "            mask2 = subclass == c2\n",
    "            print(f\"{c2}{suffix}, {loss_vec[mask & mask2].mean():0.4f}, {len(loss_vec[mask & mask2])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f4460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import torch\n",
    "import numpy as np\n",
    "def get_automatic_loss_vec(filename=None):\n",
    "    overall_loss_vec = np.ones(len(superclass)) * -1.0\n",
    "    for c in range(processor.hparams['num_classes']):\n",
    "        mask = superclass == c\n",
    "        dv_vals = -dv[mask]\n",
    "        dv_vals = dv_vals - dv_vals.min()\n",
    "        dv_vals = dv_vals/dv_vals.mean()\n",
    "        overall_loss_vec[mask] = dv_vals\n",
    "    visualize_loss_vec(overall_loss_vec)\n",
    "\n",
    "    train_indices = torch.load(processor.metrics['args']['indices_file'])['train_indices']\n",
    "    big_loss_vec = torch.ones(train_indices.max()+1) * -1\n",
    "    big_loss_vec[train_indices] = torch.tensor(overall_loss_vec).float()\n",
    "    if filename is not None:\n",
    "        torch.save(big_loss_vec, filename)\n",
    "\n",
    "def get_simple_loss_vec(upweight=2, filename=None):\n",
    "    overall_loss_vec = np.ones(len(superclass)) * -1.0\n",
    "    for c in range(processor.hparams['num_classes']):\n",
    "        mask = superclass == c\n",
    "        overall_loss_vec[mask & (dv <= 0)] = upweight\n",
    "        overall_loss_vec[mask & (dv > 0)] = 1\n",
    "    visualize_loss_vec(overall_loss_vec)\n",
    "\n",
    "    train_indices = torch.load(processor.metrics['args']['indices_file'])['train_indices']\n",
    "    big_loss_vec = torch.ones(train_indices.max()+1) * -1\n",
    "    big_loss_vec[train_indices] = torch.tensor(overall_loss_vec).float()\n",
    "    if filename is not None:\n",
    "        torch.save(big_loss_vec, filename)\n",
    "            \n",
    "def get_oracle_loss_vec(upweight=2, filename=None, balance=False):\n",
    "    overall_loss_vec = np.ones(len(superclass)) * -1.0\n",
    "    overall_loss_vec[problematic] = upweight\n",
    "    overall_loss_vec[~problematic] = 1\n",
    "    visualize_loss_vec(overall_loss_vec)\n",
    "\n",
    "    train_indices = torch.load(processor.metrics['args']['indices_file'])['train_indices']\n",
    "    big_loss_vec = torch.ones(train_indices.max()+1) * -1\n",
    "    big_loss_vec[train_indices] = torch.tensor(overall_loss_vec).float()\n",
    "    if filename is not None:\n",
    "        torch.save(big_loss_vec, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c16399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=None\n",
    "filename=os.path.join(loss_upweight_root, \"soft.pt\")\n",
    "get_automatic_loss_vec(filename=filename) # get soft upweighting\n",
    "print(\"----------------------\")\n",
    "filename=os.path.join(loss_upweight_root, \"overweight_2.pt\")\n",
    "get_simple_loss_vec(upweight=2, filename=filename)\n",
    "print(\"----------------------\")\n",
    "# filename=os.path.join(\"loss_upweight_root\", \"balanced.pt\")\n",
    "# get_simple_loss_vec(balance=True, filename=filename)\n",
    "# print(\"----------------------\")\n",
    "filename=os.path.join(loss_upweight_root, \"oracle_2.pt\")\n",
    "get_oracle_loss_vec(upweight=2, filename=filename) # oracle upweight to 2\n",
    "# # filename=os.path.join(\"loss_upweight_root\", \"oracle_balance.pt\")\n",
    "# get_oracle_loss_vec(balance=True, filename=\"oracle_balance.pt\") # oracle upweight balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52b77cb",
   "metadata": {},
   "source": [
    "# Subset Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a384dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'unlabeled'\n",
    "unlabeled_dv = processor.metrics[f'{split}_metrics']['decision_values']\n",
    "unlabeled_confs = processor.run_dict[split]['confs']\n",
    "unlabeled_superclass = processor.metrics[f'{split}_metrics']['classes'] # 0 if female, 1 if male\n",
    "unlabeled_subclass = processor.metrics[f'{split}_metrics']['spuriouses'] #1 if blond, 2 if black hair, 0 if neither\n",
    "unlabeled_problematic = np.in1d(unlabeled_subclass, classes_to_drop)\n",
    "unlabeled_pred_correct = processor.metrics[f'{split}_metrics']['ypred']\n",
    "unlabeled_correct = processor.metrics[f'{split}_metrics']['ytrue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810ced0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_problematic.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1faa553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as scipy_stats\n",
    "import pandas as pd\n",
    "rand_perm = np.arange(len(unlabeled_dv))\n",
    "np.random.shuffle(rand_perm)\n",
    "\n",
    "all_dv_inds = []\n",
    "all_conf_inds = []\n",
    "all_random_inds = []\n",
    "K_range = np.arange(25, 125, 25)\n",
    "for K in K_range:\n",
    "    dv_inds = []\n",
    "    conf_inds = []\n",
    "    random_inds = []\n",
    "    for c in range(20):\n",
    "        mask = unlabeled_superclass == c\n",
    "        masked_indices = np.arange(len(mask))[mask]\n",
    "        N = len(unlabeled_dv[mask])\n",
    "        dv_order = masked_indices[np.argsort(unlabeled_dv[mask])]\n",
    "        conf_order = masked_indices[np.argsort(unlabeled_confs[mask])]\n",
    "        random_order = masked_indices[np.argsort(rand_perm[mask])]\n",
    "        dv_inds.append(dv_order[:K])\n",
    "        conf_inds.append(conf_order[:K])\n",
    "        random_inds.append(random_order[:K])\n",
    "        \n",
    "    all_dv_inds.append(np.concatenate(dv_inds))\n",
    "    all_conf_inds.append(np.concatenate(conf_inds))\n",
    "    all_random_inds.append(np.concatenate(random_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['K'] = K_range\n",
    "df['SVM'] = [unlabeled_problematic[all_dv_inds[i]].mean() for i in range(len(K_range))]\n",
    "df['Confidence'] = [unlabeled_problematic[all_conf_inds[i]].mean() for i in range(len(K_range))]\n",
    "df['Random'] = [unlabeled_problematic[all_random_inds[i]].mean() for i in range(len(K_range))]\n",
    "df = df.melt('K', value_name='Unlabeled Fraction Problematic', var_name='Method')\n",
    "display(df)\n",
    "sns.lineplot(data=df, x='K', y='Unlabeled Fraction Problematic', hue='Method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b11ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_file = torch.load(processor.metrics['args']['indices_file'])\n",
    "u_indices = indices_file['unlabeled_indices']\n",
    "for name, order in [(\"dv\", all_dv_inds), ('confs', all_conf_inds), ('random', all_random_inds)]:\n",
    "    os.makedirs(os.path.join(subset_root, name), exist_ok=True)\n",
    "    for i in range(len(all_dv_inds)):\n",
    "        subset_indices_dict = {\n",
    "            'val_indices': indices_file['val_indices'],\n",
    "            'train_indices': torch.cat([indices_file['train_indices'],u_indices[order[i]]]),\n",
    "            'classes_to_drop': indices_file['classes_to_drop']\n",
    "        }\n",
    "        torch.save(subset_indices_dict, os.path.join(subset_root, name, f'{i}.pt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb935a",
   "metadata": {},
   "source": [
    "# Load intervention files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c99a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_range = np.arange(25, 125, 25)\n",
    "\n",
    "name_map = {\n",
    "    'dv': 'SVM Decision Value',\n",
    "    'confs': 'Confidence',\n",
    "    'baseline': 'Base Population',\n",
    "    'random': 'Random'\n",
    "}\n",
    "\n",
    "mask = np.ones(len(test_problematic)) == 1\n",
    "df = []\n",
    "for t in ['dv', 'confs', 'baseline', 'random']:\n",
    "    for i in range(len(K_range)):\n",
    "        for v in range(5):\n",
    "            if t == 'baseline':\n",
    "                is_corrects = torch.tensor(test_correct)\n",
    "            else:\n",
    "                path = os.path.join(model_root, f\"spurious_supercifar100_subset_{t}_{i}/version_{v}/metrics.pt\")\n",
    "                out = torch.load(path)\n",
    "                is_corrects = (out['test']['preds'] == out['test']['classes'])\n",
    "            flagged_acc = is_corrects[(test_pred_correct == 0) & mask].float().mean().item()\n",
    "            prob_acc = is_corrects[test_problematic & mask].float().mean().item()\n",
    "            acc = is_corrects[mask].float().mean().item()\n",
    "            df.append([name_map[t], K_range[i], flagged_acc, prob_acc, acc])\n",
    "df = pd.DataFrame(df, columns=['Order', 'K', 'Flagged', \"Problematic\", 'Accuracy'])\n",
    "display(df)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for i, w in enumerate(['Flagged', \"Problematic\", 'Accuracy']):\n",
    "    sns.lineplot(data=df, x='K', y=w, hue='Order', ax=ax[i], markers=True, \n",
    "                 hue_order=['SVM Score', 'Confidence', 'Random', 'Base Population'],\n",
    "                 palette=[BLUE, RED, ORANGE, GRAY]\n",
    "                )\n",
    "#     sns.scatterplot(data=df, x='K', y=w, hue='Method', ax=ax[i], markers=True)\n",
    "    handles, labels = ax[i].get_legend_handles_labels()\n",
    "    ax[i].legend(handles[:4], labels[:4])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,4))\n",
    "sns.lineplot(data=df, x='K', y='Problematic', hue='Order', ax=ax, markers=True, \n",
    "             hue_order=['SVM Decision Value', 'Confidence', 'Random', 'Base Population'],\n",
    "             palette=[BLUE, RED, ORANGE, GRAY]\n",
    "            )\n",
    "ax.set_xticks(K_range)\n",
    "ax.set_xlabel('K Added Images')\n",
    "ax.set_ylabel('Accuracy on Minority Subclass')\n",
    "# handles, labels = ax.get_legend_handles_labels()\n",
    "# ax.legend(handles[:4], labels[:4])\n",
    "plt.savefig('figures/spurious_cifar100/intervention.pdf', bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8b1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc187a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4f11fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(7):\n",
    "    for n in ['dv', 'confs']:\n",
    "        print(n)\n",
    "        out = torch.load(os.path.join(model_root, f\"add_30_{n}/version_{i}/metrics.pt\"))\n",
    "        print(out['args']['indices_file'])\n",
    "        i_file = torch.load(os.path.join(subset_root, n, f\"2.pt\"))\n",
    "        print(np.in1d(train_spuriouses[i_file['train_indices']], classes_to_drop).sum())\n",
    "        print((out['test']['preds'] == out['test']['classes'])[test_problematic].float().mean().item())\n",
    "        print((out['test']['preds'] == out['test']['classes'])[~test_problematic].float().mean().item())\n",
    "        print((out['test']['preds'] == out['test']['classes']).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6488b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pytorch_datasets.SuperCIFAR100(root=\"/mnt/nfs/home/saachij/datasets/cifar100\", train=True)\n",
    "config = f\"dataset_configs/supercifar100.yaml\"\n",
    "hparams, train_labels, train_spuriouses = ds_utils.get_all_beton_labels(config, 'train', \"/mnt/cfs/projects/correlated_errors/betons\", include_spurious=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7414e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.in1d(train_spuriouses,  classes_to_drop).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e3c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c3ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172afe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
